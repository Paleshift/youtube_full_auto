##### Made by Paleshift, DFRC #####

##### ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ #####
import time
import sqlite3
import datetime
import re
import json
from typing import List, Set, Optional, Tuple

##### ìœ íŠœë¸Œ ë¹„ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ 1 (yt-dlp) #####
import yt_dlp

##### ìœ íŠœë¸Œ ë¹„ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ 2 (Selenium) #####
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import (
    StaleElementReferenceException,
    ElementClickInterceptedException,
    ElementNotInteractableException,
)

##### ìœ íŠœë¸Œ ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (YouTube Data API v3) #####
import requests

##########

##### ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ê¸°ë³¸ ì„¤ì • #####

QUERY = "USA" # TODO: ì›í•˜ëŠ” ê²€ìƒ‰ì–´ë¡œ êµì²´

SEARCH_FILTER_SP = "CAASBAgBEAE%253D" # í•„í„°: ì—…ë¡œë“œ ë‚ ì§œ(ì§€ë‚œ 1ì‹œê°„) + êµ¬ë¶„(ë™ì˜ìƒ) + ì •ë ¬ê¸°ì¤€(ê´€ë ¨ì„±)

DATABASE_FILE = "youtube_data.db" # ê²€ìƒ‰ ê²°ê³¼ê°€ ì €ì¥ë  DBì˜ íŒŒì¼ëª…

SCROLL_PAUSE = 2.0 # í•œ ë²ˆ ìŠ¤í¬ë¡¤ ë‚´ë¦° í›„ 2ì´ˆ ëŒ€ê¸°
MAX_SCROLL_TRIES = 100 # ìµœëŒ€ 100ë²ˆê¹Œì§€ë§Œ ìŠ¤í¬ë¡¤í•˜ê³  ì •ì§€

KST = datetime.timezone(datetime.timedelta(hours=9)) # ëª¨ë“  ì‹œê°„ì€ KSTë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥

YOUTUBE_API_KEY = "YouTube Data API v3" # TODO: ë³¸ì¸ì˜ YouTube Data API v3 í‚¤ë¡œ êµì²´

##########

##### DB ì´ˆê¸°í™” #####

conn = sqlite3.connect(DATABASE_FILE) # DB íŒŒì¼ì— ì—°ê²°
cursor = conn.cursor() # SQL ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì¤„ Cursor ìƒì„± 

cursor.execute("""
CREATE TABLE IF NOT EXISTS videos (
    id TEXT PRIMARY KEY, 
    title TEXT,          
    channel TEXT,        
    publish_time TEXT,   
    description TEXT,    
    duration TEXT,       
    status TEXT,         
    url TEXT             
);
""") # videos í…Œì´ë¸”(ì˜ìƒì˜ vid, ì¤‘ë³µ ë¶ˆê°€ / ì˜ìƒì˜ ì œëª© / ì˜ìƒì„ ì—…ë¡œë“œí•œ ì±„ë„ì˜ ì´ë¦„ / ì˜ìƒì„ ì—…ë¡œë“œí•œ ì‹œê°„ / ì˜ìƒì— ëŒ€í•œ ì„¤ëª… / ì˜ìƒì˜ ê¸¸ì´ / ì˜ìƒì˜ ì¢…ë¥˜ / ì˜ìƒì˜ url) 

cursor.execute("""
CREATE TABLE IF NOT EXISTS comments (
    video_id TEXT,                              
    comment TEXT,                               
    author_name TEXT,                           
    author_channel_id TEXT,                     
    author_channel_url TEXT,                    
    comment_time_kst TEXT,                      
    FOREIGN KEY(video_id) REFERENCES videos(id)
);
""") # comments í…Œì´ë¸”(ëŒ“ê¸€ì´ ì†í•´ìˆëŠ” ì˜ìƒì˜ vid / ëŒ“ê¸€ ë‚´ìš© / ëŒ“ê¸€ ì‘ì„±ì ì±„ë„ì˜ ì´ë¦„ / ëŒ“ê¸€ ì‘ì„±ì ì±„ë„ì˜ id / ëŒ“ê¸€ ì‘ì„±ì ì±„ë„ì˜ url / ëŒ“ê¸€ì„ ì‘ì„±í•œ ì‹œê°„ / videos í…Œì´ë¸”ì˜ id columnì„ ì°¸ì¡°í•´ videos_idë¥¼ foreign keyë¡œ ì„¤ì •)

cursor.execute("""
CREATE TABLE IF NOT EXISTS video_raw (
    id TEXT PRIMARY KEY,                  
    raw_json TEXT,                        
    FOREIGN KEY(id) REFERENCES videos(id)
);
""") # video_raw í…Œì´ë¸”(ì˜ìƒì˜ vid, ì¤‘ë³µ ë¶ˆê°€ / JSON ì›ë³¸ í…ìŠ¤íŠ¸ / videos í…Œì´ë¸”ì˜ id columnì„ ì°¸ì¡°í•´ idë¥¼ foreign keyë¡œ ì„¤ì •)

conn.commit()

##### 3ê°œì˜ í…Œì´ë¸” (videos, comments, video_raw) ê°ê°ì— ìˆ˜ì • ì‚¬í•­ ê¸°ë¡ì„ ìœ„í•œ column ì¶”ê°€ #####

##### videos.revised_contents: videos í…Œì´ë¸”ì˜ ìˆ˜ì • ì‚¬í•­ ê¸°ë¡ #####
##### video_raw.revised_count: video_raw í…Œì´ë¸”ì˜ ìˆ˜ì • ì‚¬í•­ ë°œìƒ íšŸìˆ˜ ê¸°ë¡ #####
##### comments.revised_contents: comments í…Œì´ë¸”ì˜ ìˆ˜ì • ì‚¬í•­ ê¸°ë¡ #####

# í…Œì´ë¸”ì— íŠ¹ì • columnì´ ì—†ìœ¼ë©´ í•´ë‹¹ columnì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def _add_column_if_missing(table_name: str, column_def: str) -> None:
    col_name = column_def.split()[0] # Ex) revised_contents TEXT --> revised_contents
    cursor.execute(f"PRAGMA table_info({table_name})") # í˜„ì¬ í…Œì´ë¸”ì˜ column ì •ë³´ ì¡°íšŒ
    cols = [r[1] for r in cursor.fetchall()] # ì¡´ì¬í•˜ëŠ” column ì´ë¦„ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ ìƒì„± 
    if col_name not in cols: # ë§Œì•½ íŠ¹ì • columnì´ ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ í•´ë‹¹ columnì„ ì¶”ê°€
        try:
            cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {column_def}")
            conn.commit()
        except Exception:
            pass # ì‘ì—…ì´ ì‹¤íŒ¨í•´ë„ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì§€ ì•Šê²Œ ê·¸ëƒ¥ ë„˜ì–´ê°

# ê° í…Œì´ë¸”ì— ìˆì–´ì•¼ í•  column
_add_column_if_missing("videos", "revised_contents TEXT")
_add_column_if_missing("video_raw", "revised_count INTEGER DEFAULT 0")
_add_column_if_missing("comments", "revised_contents TEXT")

##########

##### Selenium ì„¤ì • #####

chrome_options = Options()
chrome_options.add_argument("--headless=new")     # í™”ë©´ì„ ë„ìš°ì§€ ì•Šê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
chrome_options.add_argument("--disable-gpu")      # GPU ê°€ì† ë„ê¸°
chrome_options.add_argument("--no-sandbox")       # ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„  ë³´ì•ˆ ìƒŒë“œë°•ìŠ¤ ë„ê¸°

driver = webdriver.Chrome(options=chrome_options) # ìœ„ ì„¤ì •ëŒ€ë¡œ í¬ë¡¬ ì‹¤í–‰

##### ìœ í‹¸ í•¨ìˆ˜ #####

def build_search_url(query: str) -> str: # ê²€ìƒ‰ì–´ì™€ í•„í„°ë¥¼ í•©ì³ ìœ íŠœë¸Œ ê²€ìƒ‰ URL ìƒì„±
    return f"https://www.youtube.com/results?search_query={query}&sp={SEARCH_FILTER_SP}"


def get_publish_time_kst(info: dict) -> str: # yt-dlpë¥¼ í†µí•´ ì–»ì€ JSON ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì—…ë¡œë“œ ì‹œê°„ì„ ì°¾ì•„ KSTë¡œ ë³€í™˜
    dt = None

    # ë¨¼ì € ì •í™•í•œ ì´ˆ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
    ts = info.get("timestamp") or info.get("release_timestamp")
    if ts:
        try: # UTC ì‹œê°„ ê°ì²´ë¡œ ë³€í™˜
            dt = datetime.datetime.fromtimestamp(int(ts), tz=datetime.timezone.utc)
        except Exception:
            dt = None

    # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ ë‚ ì§œ ë¬¸ìì—´ í™•ì¸
    if dt is None:
        upload_date = info.get("upload_date")
        if upload_date:
            try:
                dt = datetime.datetime.strptime(upload_date, "%Y%m%d")
                dt = dt.replace(tzinfo=datetime.timezone.utc)
            except Exception:
                dt = None

    if dt is None:
        return "" # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

    # ì°¾ì€ ì‹œê°„ì„ KSTë¡œ ë³€í™˜í•´ ë¬¸ìì—´ë¡œ ë°˜í™˜
    return dt.astimezone(KST).strftime("%Y-%m-%dT%H:%M:%S%z")


def classify_video_type(info: dict) -> str: # ì˜ìƒì˜ ì¢…ë¥˜ë¥¼ ë¶„ë¥˜
    live_status = info.get("live_status")
    if info.get("is_live") or live_status == "is_live": # ë¼ì´ë¸Œ ë°©ì†¡ ì—¬ë¶€
        return "live"
    if live_status in ("is_upcoming", "upcoming"): # ë¼ì´ë¸Œ ë°©ì†¡ ì˜ˆì • ì—¬ë¶€
        return "upcoming_live"
    if info.get("was_live") or live_status in ("was_live", "post_live"): # ë¼ì´ë¸Œ ë°©ì†¡ ì¢…ë£Œ ì—¬ë¶€
        return "live_archive"

    # ì˜ìƒì˜ ê¸¸ì´ê°€ 1ë¶„ ë¯¸ë§Œì´ë©´ ì‡¼ì¸ ë¡œ ë¶„ë¥˜
    duration = int(info.get("duration") or 0)
    if duration < 60:
        return "short"

    # ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ ì˜ìƒìœ¼ë¡œ ë¶„ë¥˜
    return "vod"


def iso8601_to_kst(dt_str: str) -> str: # YouTube Data API v3ì˜ publishedAtì´ ë°˜í™˜í•´ì£¼ëŠ” ì‹œê°„(ISO8601, UTC)ì„ íŒŒì‹±í•´ KST ë¬¸ìì—´ë¡œ ë³€í™˜
    if not dt_str:
        return ""
    try:
        # Z(UTC)ë¥¼ +00:00ìœ¼ë¡œ ë°”ê¿” fromisoformat ì‚¬ìš©
        dt = datetime.datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
        return dt.astimezone(KST).strftime("%Y-%m-%dT%H:%M:%S%z")
    except Exception:
        return ""


def _get_next_revision_number(revised_contents: str) -> int: # ê¸°ì¡´ revised_contentsì˜ ë¬¸ìì—´ì„ ë³´ê³  ë‹¤ìŒ ìˆ˜ì • ë²ˆí˜¸ ê³„ì‚°
    if not revised_contents or not revised_contents.strip():
        return 1
    # ì •ê·œì‹ìœ¼ë¡œ "ìˆ«ì." íŒ¨í„´ì˜ ê°œìˆ˜ë¥¼ ì…ˆ
    rounds = re.findall(r"\b(\d+)\.", revised_contents)
    try:
        return len(rounds) + 1
    except Exception:
        return 1


def _append_revision(revised_contents: str, changes: List[str], revision_number: int) -> str: # ë³€ê²½ ì‚¬í•­ë“¤ì„ ê¸°ì¡´ revised_contentsì˜ ë¬¸ìì—´ì— ì¶”ê°€
    if not changes:
        return revised_contents
    new_entry = f"{revision_number}. " + ", ".join(changes)
    if not revised_contents or not revised_contents.strip():
        return new_entry
    else: # ê° ë¼ìš´ë“œëŠ” ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
        return f"{revised_contents} {new_entry}"


def _update_video_raw_revised_count(video_id: str, raw_json_str: str, changed: bool) -> None: # video_raw í…Œì´ë¸”ì˜ raw_jsonê³¼ revised_countë¥¼ ê°±ì‹ 
    cursor.execute("SELECT revised_count FROM video_raw WHERE id = ?", (video_id,))
    row = cursor.fetchone()
    if row: # UPDATEí•˜ê¸°
        current_count = row[0] or 0
        new_count = current_count + 1 if changed else current_count
        cursor.execute(
            "UPDATE video_raw SET raw_json = ?, revised_count = ? WHERE id = ?",
            (raw_json_str, new_count, video_id),
        )
    else:   # INSERTí•˜ê¸°
        init_count = 1 if changed else 0
        cursor.execute(
            "INSERT INTO video_raw (id, raw_json, revised_count) VALUES (?, ?, ?)",
            (video_id, raw_json_str, init_count),
        )
    conn.commit()


def scroll_and_collect_video_ids(query: str) -> List[str]: # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•´ ì˜ìƒ IDë§Œ ìˆ˜ì§‘
    url = build_search_url(query)
    print(f"ğŸ” ê²€ìƒ‰ URL: {url}")
    driver.get(url) # ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ì— ê²€ìƒ‰ URLì„ ì…ë ¥í•´ ì´ë™
    time.sleep(3)   # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ê°€ ëœ° ë•Œê¹Œì§€ 3ì´ˆ ëŒ€ê¸°

    video_ids: Set[str] = set() # ì¤‘ë³µë˜ëŠ” IDëŠ” ìë™ìœ¼ë¡œ ì œê±°ë˜ë„ë¡ ì§‘í•©(Set) ì‚¬ìš©

    # ìŠ¤í¬ë¡¤ ë†’ì´ë¥¼ ì¸¡ì •í•´ ë” ë‚´ë ¤ê°ˆ ê³³ì´ ìˆëŠ”ì§€ í™•ì¸  
    last_height = driver.execute_script("return document.documentElement.scrollHeight")
    last_count = 0
    stable_rounds = 0

    # ìµœëŒ€ MAX_SCROLL_TRIES ë§Œí¼ ë°˜ë³µí•˜ë©° ìŠ¤í¬ë¡¤
    for i in range(MAX_SCROLL_TRIES):
        # í™”ë©´ì— ë³´ì´ëŠ” ì˜ìƒë“¤ì˜ ë§í¬ë¥¼ íƒìƒ‰
        elements = driver.find_elements(By.XPATH, '//a[@id="video-title"]')
        for elem in elements:
            href = elem.get_attribute("href")
            # ë§í¬ê°€ ì¡´ì¬í•˜ê³  "/watch?v="ì˜ í˜•ì‹ì´ë¼ë©´ vid ì¶”ì¶œ
            if href and "/watch?v=" in href:
                vid = href.split("watch?v=")[1].split("&")[0]
                video_ids.add(vid)

        print(f" --> ìŠ¤í¬ë¡¤ {i+1}íšŒì°¨ / ìˆ˜ì§‘ëœ ì˜ìƒ ìˆ˜: {len(video_ids)}ê°œ")

        # ìë°”ìŠ¤í¬ë¦½íŠ¸ë¡œ í™”ë©´ì„ ë§¨ ì•„ë˜ë¡œ ë‚´ë¦¼
        driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
        time.sleep(SCROLL_PAUSE)

        # ìŠ¤í¬ë¡¤ í›„ ë†’ì´ì™€ ì˜ìƒ ê°œìˆ˜ì˜ ë³€í™” í™•ì¸ 
        new_height = driver.execute_script("return document.documentElement.scrollHeight")
        new_count = len(video_ids)

        # ë†’ì´ì™€ ì˜ìƒ ê°œìˆ˜ì˜ ë³€í™”ê°€ ì—†ê³ , ì´ë¥¼ í¬í•¨í•´ 3ë²ˆ ì—°ì† ë³€í™”ê°€ ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤ ì¢…ë£Œ
        if new_height == last_height and new_count == last_count:
            stable_rounds += 1
            if stable_rounds >= 3:
                print("ğŸ“Œ ë” ì´ìƒ ë¡œë”©ë˜ëŠ” ì˜ìƒ ì—†ìŒ --> ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
                break
        else:
            stable_rounds = 0 # ë³€í™”ê°€ ìˆìœ¼ë©´ ì´ˆê¸°í™”

        last_height = new_height
        last_count = new_count

    print(f"ğŸ“¦ ìµœì¢… ìˆ˜ì§‘ëœ ì˜ìƒ ìˆ˜: {len(video_ids)}ê°œ")
    return list(video_ids) # Setì„ Listë¡œ ë³€í™˜í•´ ë°˜í™˜


def fetch_and_store_video_metadata(video_id: str) -> bool: # yt-dlpë¥¼ í†µí•´ DBì— ì˜ìƒ ë©”íƒ€ë°ì´í„° ì €ì¥
    video_url = f"https://www.youtube.com/watch?v={video_id}"

    # yt-dlp ì„¤ì •: ë‹¤ìš´ë¡œë“œëŠ” í•˜ì§€ ì•Šê³  JSON ì •ë³´ë§Œ ê°€ì ¸ì˜´
    ydl_opts = {
        "quiet": True,
        "skip_download": True,
        "forcejson": True,
        "extract_flat": False,
    }

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)
    except Exception as e:
        print(f"[Error! ì˜ìƒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨] {video_id}: {e}")
        return False

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    title = info.get("title", "") or ""
    channel = info.get("channel", "") or ""
    description = info.get("description", "") or ""
    duration_sec = info.get("duration", 0) or 0

    status = classify_video_type(info)
    publish_time = get_publish_time_kst(info)

    minutes, seconds = divmod(duration_sec, 60)
    duration_str = f"{minutes}ë¶„ {seconds}ì´ˆ"

    # JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    raw_json_str = json.dumps(info, ensure_ascii=False)

    # DBì— í•´ë‹¹ ì˜ìƒì´ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    cursor.execute(
        "SELECT id, title, channel, publish_time, description, duration, status, url, revised_contents FROM videos WHERE id = ?",
        (video_id,),
    )
    existing = cursor.fetchone()

    # ìƒˆë¡œ ì €ì¥í•˜ë ¤ëŠ” ê°’ë“¤
    new_values = {
        "title": title,
        "channel": channel,
        "publish_time": publish_time,
        "description": description,
        "duration": duration_str,
        "status": status,
        "url": video_url,
    }

    changed = False
    revision_changes: List[str] = []

    # í•´ë‹¹ ì˜ìƒì´ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´ ë³€ê²½ ì‚¬í•­ ë¹„êµ
    if existing:
        # existing = (id, title, channel, publish_time, description, duration, status, url, revised_contents)
        old_values = {
            "title": existing[1] or "",
            "channel": existing[2] or "",
            "publish_time": existing[3] or "",
            "description": existing[4] or "",
            "duration": existing[5] or "",
            "status": existing[6] or "",
            "url": existing[7] or "",
        }

        # í•˜ë‚˜í•˜ë‚˜ ë³€ê²½ ì‚¬í•­ í™•ì¸
        for key in new_values:
            old_val = old_values.get(key, "")
            new_val = new_values[key] or ""
            if old_val != new_val:
                changed = True
                # ë³€ê²½ ì‚¬í•­ ê¸°ë¡
                revision_changes.append(f"({key}: {old_val} --> {new_val})")

        # ë³€ê²½ ì‚¬í•­ì´ ìˆìœ¼ë©´ revised_contents ê°±ì‹ 
        old_revised = existing[8] or ""
        if changed:
            revision_number = _get_next_revision_number(old_revised)
            new_revised = _append_revision(old_revised, revision_changes, revision_number)
        else:
            new_revised = old_revised
        
        # DB ê°±ì‹ 
        cursor.execute(
            "UPDATE videos SET title = ?, channel = ?, publish_time = ?, description = ?, duration = ?, status = ?, url = ?, revised_contents = ? WHERE id = ?",
            (
                new_values["title"],
                new_values["channel"],
                new_values["publish_time"],
                new_values["description"],
                new_values["duration"],
                new_values["status"],
                new_values["url"],
                new_revised,
                video_id,
            ),
        )
    else:
        # ì²˜ìŒ ë³´ëŠ” ì˜ìƒì´ë¼ë©´ ì €ì¥
        revision_changes = []
        new_revised = ""
        cursor.execute(
            "INSERT INTO videos (id, title, channel, publish_time, description, duration, status, url, revised_contents) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (
                video_id,
                new_values["title"],
                new_values["channel"],
                new_values["publish_time"],
                new_values["description"],
                new_values["duration"],
                new_values["status"],
                new_values["url"],
                new_revised,
            ),
        )
        changed = False
    
    # video_raw í…Œì´ë¸” ì—…ë°ì´íŠ¸
    _update_video_raw_revised_count(video_id, raw_json_str, changed)
    conn.commit()
    
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {video_id} / {title}")
    print(f"  --> publish_time(KST): {publish_time}, status: {status}")
    if changed and revision_changes:
        print(f"  --> ë³€ê²½ì‚¬í•­: {', '.join(revision_changes)}")
    return True


def fetch_comments_via_api(video_id: str) -> List[Tuple[str, str, str, str, str, str]]: # YouTube Data API v3(commentThreads)ë¥¼ í†µí•´ ëŒ“ê¸€ê³¼ ë‹µê¸€ ìˆ˜ì§‘
    rows: List[Tuple[str, str, str, str, str, str]] = []
    base_url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {
        "key": YOUTUBE_API_KEY,
        "part": "snippet,replies", # snippet(ëŒ“ê¸€), replies(ë‹µê¸€)
        "videoId": video_id,
        "maxResults": 100,
        "textFormat": "plainText", 
        "order": "time",           # ìµœì‹ ìˆœ
    }

    while True:
        resp = requests.get(base_url, params=params) # request ì „ì†¡
        if resp.status_code != 200:
            print(f"[Error! commentThreads í˜¸ì¶œ ì‹¤íŒ¨ ({resp.status_code}): {resp.text[:200]}]")
            break

        data = resp.json()
        items = data.get("items", []) # ëŒ“ê¸€ ëª©ë¡

        for item in items:
            # ëŒ“ê¸€ ì²˜ë¦¬
            try:
                top = item["snippet"]["topLevelComment"]["snippet"]
            except KeyError:
                continue

            # ì •ë³´ ì¶”ì¶œ
            comment_text = top.get("textDisplay") or top.get("textOriginal") or ""
            author_name = top.get("authorDisplayName", "")

            author_channel_id = ""
            author_channel_url = ""
            ch_obj = top.get("authorChannelId") or {}
            if isinstance(ch_obj, dict):
                author_channel_id = ch_obj.get("value", "") or ""
            if author_channel_id:
                author_channel_url = f"https://www.youtube.com/channel/{author_channel_id}"

            published_at = top.get("publishedAt", "")
            comment_time_kst = iso8601_to_kst(published_at)

            # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            rows.append(
                (
                    video_id,
                    comment_text,
                    author_name,
                    author_channel_id,
                    author_channel_url,
                    comment_time_kst,
                )
            )

            # ë‹µê¸€ ì²˜ë¦¬
            replies = (item.get("replies") or {}).get("comments") or []
            for rep in replies:
                rs = rep.get("snippet", {})

                # ì •ë³´ ì¶”ì¶œ
                r_text = rs.get("textDisplay") or rs.get("textOriginal") or ""
                r_author_name = rs.get("authorDisplayName", "")

                r_author_channel_id = ""
                r_author_channel_url = ""
                r_ch_obj = rs.get("authorChannelId") or {}
                if isinstance(r_ch_obj, dict):
                    r_author_channel_id = r_ch_obj.get("value", "") or ""
                if r_author_channel_id:
                    r_author_channel_url = f"https://www.youtube.com/channel/{r_author_channel_id}"

                r_published_at = rs.get("publishedAt", "")
                r_comment_time_kst = iso8601_to_kst(r_published_at)

                rows.append(
                    (
                        video_id,
                        r_text,
                        r_author_name,
                        r_author_channel_id,
                        r_author_channel_url,
                        r_comment_time_kst,
                    )
                )

        # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€(ëŒ“ê¸€ì´ 100ê°œê°€ ë„˜ëŠ”ì§€) í™•ì¸
        page_token = data.get("nextPageToken")
        if not page_token:
            break # ì—†ìœ¼ë©´ ì¢…ë£Œ
        params["pageToken"] = page_token # ë‹¤ìŒ í˜ì´ì§€ í† í° ì„¤ì • í›„ ë£¨í”„

    return rows


def scroll_and_collect_all_comments(video_id: str) -> None: # ìˆ˜ì§‘í•œ ëŒ“ê¸€ê³¼ ë‹µê¸€ì„ DBì— ì €ì¥
    print(f"  --> ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘ (YouTube Data API v3): {video_id}")

    rows = fetch_comments_via_api(video_id)
    print(f"  --> APIë¡œ ê°€ì ¸ì˜¨ ëŒ“ê¸€(+ë‹µê¸€) ê°œìˆ˜: {len(rows)}ê°œ")

    for row in rows:
        vid, comment_text, author_name, author_channel_id, author_channel_url, comment_time_kst = row
        # ëŒ“ê¸€ì€ ê³ ìœ í•œ IDê°€ ë”°ë¡œ ì—†ê¸°ì— [ì˜ìƒID + ì‘ì„±ìì±„ë„ID + ì‘ì„±ì‹œê°„]ì„ ë³µí•©í‚¤ë¡œ ì‚¬ìš©í•´ ì‹ë³„
        cursor.execute(
            "SELECT comment, author_name, author_channel_url, comment_time_kst, revised_contents FROM comments WHERE video_id = ? AND author_channel_id = ? AND comment_time_kst = ?",
            (vid, author_channel_id or "", comment_time_kst),
        )
        existing = cursor.fetchone()
        new_values = {
            "comment": comment_text or "",
            "author_name": author_name or "",
            "author_channel_url": author_channel_url or "",
            "comment_time_kst": comment_time_kst or "",
        }
        changed = False
        revision_changes: List[str] = []
        if existing:
            # existing = (comment, author_name, author_channel_url, comment_time_kst, revised_contents)
            old_values = {
                "comment": existing[0] or "",
                "author_name": existing[1] or "",
                "author_channel_url": existing[2] or "",
                "comment_time_kst": existing[3] or "",
            }

            # í•˜ë‚˜í•˜ë‚˜ ë³€ê²½ ì‚¬í•­ í™•ì¸
            for key in new_values:
                old_val = old_values.get(key, "")
                new_val = new_values[key]
                if old_val != new_val:
                    changed = True
                    # ë³€ê²½ ì‚¬í•­ ê¸°ë¡
                    revision_changes.append(f"({key}: {old_val} --> {new_val})")
            old_revised = existing[4] or ""
            if changed:
                revision_number = _get_next_revision_number(old_revised)
                new_revised = _append_revision(old_revised, revision_changes, revision_number)
                
                # DB ê°±ì‹ 
                cursor.execute(
                    "UPDATE comments SET comment = ?, author_name = ?, author_channel_url = ?, comment_time_kst = ?, revised_contents = ? WHERE video_id = ? AND author_channel_id = ? AND comment_time_kst = ?",
                    (
                        new_values["comment"],
                        new_values["author_name"],
                        new_values["author_channel_url"],
                        new_values["comment_time_kst"],
                        new_revised,
                        vid,
                        author_channel_id or "",
                        comment_time_kst,
                    ),
                )
            
        else:
            # ì²˜ìŒ ë³´ëŠ” ëŒ“ê¸€ì´ë¼ë©´ ì €ì¥
            cursor.execute(
                "INSERT INTO comments (video_id, comment, author_name, author_channel_id, author_channel_url, comment_time_kst, revised_contents) VALUES (?, ?, ?, ?, ?, ?, ?)",
                (
                    vid,
                    new_values["comment"],
                    new_values["author_name"],
                    author_channel_id or "",
                    new_values["author_channel_url"],
                    new_values["comment_time_kst"],
                    "",
                ),
            )
    conn.commit()
    print(f"ğŸ’¬ [{video_id}] ìµœì¢… ëŒ“ê¸€(+ë‹µê¸€) {len(rows)}ê°œ ì €ì¥ ì™„ë£Œ")

##### ì „ì²´ íŒŒì´í”„ë¼ì¸ #####

def run_pipeline(query: str):
    video_ids = scroll_and_collect_video_ids(query)

    for idx, vid in enumerate(video_ids, 1):
        print(f"\n====== {idx} / {len(video_ids)} ì²˜ë¦¬ ì¤‘: {vid} ======")
        ok = fetch_and_store_video_metadata(vid)
        if not ok:
            continue
        try:
            scroll_and_collect_all_comments(vid)
        except Exception as e:
            print(f"[Error! ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨] {vid}: {e}")

##### main #####

if __name__ == "__main__":
    try:
        run_pipeline(QUERY)
    finally:
        driver.quit()
        conn.close()
        print("\nâœ… ì¢…ë£Œ")